{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imported Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.chdir('C:/Users/emrcaah/chanterelle-experimentation-master/chanterelle-experimentation-master/notebooks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math as mt\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sqlalchemy import create_engine\n",
    "from utils import *\n",
    "import datetime as dt\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import pdist\n",
    "import matplotlib as mpl\n",
    "## agg backend is used to create plot as a .png file\n",
    "#mpl.use('agg')\n",
    "from matplotlib.ticker import MultipleLocator, FormatStrFormatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"response_time_max\"\n",
    "dataset = \"pm_transformed-fixed-sp.csv\"\n",
    "#dataset = \"load40and90-sim.csv\"\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spadata = \"spadata.csv\"\n",
    "spadatablob = \"spadata-blob.csv\"\n",
    "spadatatrucks = \"spadata-empty-semi-trucks.csv\"\n",
    "spadataexpensive = \"spadata-expensive-db.csv\"\n",
    "spadatastifle = \"spadata-stifle.csv\"\n",
    "spadatajam = \"spadata-traffic-jam.csv\"\n",
    "spadatacont = \"spadata-continuous.csv\"\n",
    "spadatahic = \"spadata-hiccups.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_outliers(anomaly_counter):\n",
    "    \"\"\"We label as outliner only the rows with anomaly_counter equals to -1\"\"\"\n",
    "    if anomaly_counter == -1:\n",
    "        return \"Outlier\" \n",
    "    else: \n",
    "        return \"Inliner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_baseline(mean, std):\n",
    "    \"\"\"We multiply by 0.6 assuming that the system without queuing has a mean response time 60% lower (T = mean * (1-load))\"\"\"\n",
    "    return (mean * 0.6 + 3*std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_counters(counter, baseline):\n",
    "    \"\"\"We compare a measurement with the baseline of a given counter\"\"\"\n",
    "    if counter >= baseline:\n",
    "        return \"Fail\" \n",
    "    else:\n",
    "        return \"Pass\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistic_df = pd.read_csv(\"pm_transformed-fixed-sp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spa_df = pd.read_csv(spadata)\n",
    "spablob_df = pd.read_csv(spadatablob)\n",
    "spatrucks_df = pd.read_csv(spadatatrucks)\n",
    "spaexpensive_df = pd.read_csv(spadataexpensive)\n",
    "spastifle_df = pd.read_csv(spadatastifle)\n",
    "spajam_df = pd.read_csv(spadatajam)\n",
    "spacont_df = pd.read_csv(spadatacont)\n",
    "spahic_df = pd.read_csv(spadatahic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for load in lb_detection_df.load.unique():\n",
    " #   if (load == 90):\n",
    "       # for counter in lb_detection_df.loc[lb_detection_df.counter_name == service, 'counter_name'].unique():\n",
    "figure = plt.figure\n",
    "ax = plt.gca()\n",
    "#ax.scatter(spa_df['ndistance'], spa_df['slope'], color='red')\n",
    "\n",
    "\n",
    "ax.scatter(spablob_df['ndistance'], spablob_df['Slope'], color='blue',label='the blob')\n",
    "ax.scatter(spatrucks_df['ndistance'], spatrucks_df['Slope'], color='green',label='empty semi trucks')\n",
    "ax.scatter(spaexpensive_df['ndistance'], spaexpensive_df['Slope'], color='red',label='expensive db calls')\n",
    "ax.scatter(spastifle_df['ndistance'], spastifle_df['Slope'], color='purple',label='the stifle')\n",
    "ax.scatter(spajam_df['ndistance'], spajam_df['Slope'], color='orange',label='traffic jam')\n",
    "ax.scatter(spacont_df['ndistance'], spacont_df['Slope'], color='black',label='continuous violated requirements')\n",
    "ax.scatter(spahic_df['ndistance'], spahic_df['Slope'], color='magenta',label='application hiccups')\n",
    "ax.set_xlabel(' normalized distance > 1 failed performance requirement baseline')\n",
    "ax.set_ylabel('slope > 0.1 failed scalability trend')\n",
    "ax.set_ylim(-10,550)\n",
    "        #ax.set_title(\"{} vs {}\".format(x_col, y_col))\n",
    "majorLocator   = MultipleLocator(5)\n",
    "majorFormatter = FormatStrFormatter('%d')\n",
    "minorLocator   = MultipleLocator(1)\n",
    "plt.axhline(0.1,color='black')  #horizontal line\n",
    "plt.axvline(1,color='black')  #vertical line\n",
    "#plt.legend()\n",
    "#ax.legend(fancybox=True, framealpha=1, shadow=True, borderpad=1, loc='upper left')\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.05),\n",
    "          ncol=2, fancybox=True, shadow=True)\n",
    "#plt.show()\n",
    "plt.grid()\n",
    "plt.savefig('spa_all.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for load in lb_detection_df.load.unique():\n",
    " #   if (load == 90):\n",
    "       # for counter in lb_detection_df.loc[lb_detection_df.counter_name == service, 'counter_name'].unique():\n",
    "figure = plt.figure\n",
    "ax = plt.gca()\n",
    "ax.scatter(spa_df['ndistance'], spa_df['slope'], color='red')\n",
    "\n",
    "\n",
    "ax.set_xlabel(' normalized distance > 1 failed performance requirement baseline')\n",
    "ax.set_ylabel('slope > 0.1 failed scalability trend')\n",
    "ax.set_ylim(-10,400)\n",
    "        #ax.set_title(\"{} vs {}\".format(x_col, y_col))\n",
    "majorLocator   = MultipleLocator(5)\n",
    "majorFormatter = FormatStrFormatter('%d')\n",
    "minorLocator   = MultipleLocator(1)\n",
    "plt.axhline(0.1,color='black')  #horizontal line\n",
    "plt.axvline(1,color='black')  #vertical line\n",
    "\n",
    "\n",
    "#plt.show()\n",
    "plt.grid()\n",
    "plt.savefig('spa_90.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution (probability of S)\n",
    "statistic_df[\"load\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the probability of a giving load\n",
    "load_probabilities_df = pd.DataFrame(statistic_df.groupby(['load']).instance.count().rename(\"load_probability\"))\n",
    "print(load_probabilities_df)\n",
    "load_probabilities_df.load_probability /= load_probabilities_df.load_probability.sum()\n",
    "statistic_df = pd.merge(statistic_df, load_probabilities_df, left_on=['load'], right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(load_probabilities_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate probability of executing any of the operations by summing up all operations as \n",
    "# the denominator and the operation counter as the numerator\n",
    "probabilities_df = pd.DataFrame(statistic_df.groupby(['counter_name']).instance.count().rename(\"activation_probability\"))\n",
    "#probabilities_df = pd.DataFrame(probabilities_df/probabilities_df.groupby(level=[0, 1]).transform(\"sum\"))\n",
    "probabilities_df.activation_probability /= probabilities_df.activation_probability.sum()\n",
    "probabilities_df = probabilities_df.reset_index()\n",
    "statistic_df = pd.merge(statistic_df, probabilities_df, on=['counter_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline calculation and assessment\n",
    "assessment_df = pd.DataFrame(statistic_df.groupby(['counter_name'])[metric].agg(['mean', 'std']))\n",
    "\n",
    "assessment_df['baseline'] = assessment_df.apply(lambda x: calc_baseline(x[\"mean\"], x[\"std\"]), axis=1)\n",
    "print(assessment_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "statistic_df = pd.merge(statistic_df, assessment_df[['baseline']], \n",
    "                        left_on='counter_name', right_index=True).reset_index(drop=True)\n",
    "statistic_df[\"assessment\"] = statistic_df.apply(lambda x: eval_counters(x[metric], x[\"baseline\"]), axis=1)\n",
    "statistic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate fraction of successful service execution * the probability activation of the services\n",
    "s_df = statistic_df.groupby(['load', 'load_probability', 'counter_name', 'activation_probability', 'assessment']).instance.count().rename(\"s\")\n",
    "s_df = pd.DataFrame(s_df/s_df.groupby(level=[0, 1, 2]).transform(\"sum\"))\n",
    "s_df = s_df.reset_index()\n",
    "s_df = s_df[s_df.assessment == 'Pass'].sort_values(['load','s'], ascending=[True, False]).reset_index(drop=True)\n",
    "s_df = s_df.drop(columns=['assessment'])\n",
    "groupby_dict = {\"activation_probability\":\"s\", \n",
    "           \"s\":\"s\"} \n",
    "s_df = s_df.set_index(['load', 'load_probability', 'counter_name'])\n",
    "s_df = s_df.groupby(groupby_dict, axis = 1).prod().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add analysis timestamp\n",
    "statistic_df['analysis_timestamp'] = dt.datetime.today()\n",
    "#domain_metric_df['analysis_timestamp'] = dt.datetime.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "lb_detection_df = statistic_df.copy()\n",
    "lb_detection_df['anomaly'] = pd.Series()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lb_detection_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(statistic_df.groupby(['counter_name','load']).agg({metric:'max','baseline':'mean'}))\n",
    "df.columns.name=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(statistic_df.groupby(['counter_name','load']).agg({metric:'max','baseline':'mean'}))\n",
    "df.columns.name=None\n",
    "df=df.reset_index()\n",
    "df['distance'] = df.baseline - df[metric]\n",
    "df['ndistance'] = 2*df[metric]/(df.baseline+df[metric])\n",
    "df['assessment'] = df.distance.apply(lambda x: False if (x>0) else True)\n",
    "df['binary'] = df.distance.apply(lambda x: 0 if (x>0) else 1)\n",
    "\n",
    "bs_df=pd.DataFrame(df.groupby(['counter_name']).binary.sum())\n",
    "\n",
    "df = pd.merge(df, bs_df, on=\"counter_name\")\n",
    "df = df.rename(columns={\"binary_x\":\"binary\", \"binary_y\":\"sbinary\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#measurement < baseline ndistance -> 0, measurement = baseline ndistance = 0.5\n",
    "#measurement >> baseline ndistance -> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute slope and concatenate to df\n",
    "#create slope df\n",
    "slope_df = df.copy()\n",
    "slope_df['slope'] = pd.Series()\n",
    "for service in lb_detection_df.counter_name.unique():\n",
    "    for counter in lb_detection_df.loc[lb_detection_df.counter_name == service, 'counter_name'].unique():\n",
    "        x = lb_detection_df.loc[(lb_detection_df.counter_name == counter), 'load']\n",
    "        y = lb_detection_df.loc[(lb_detection_df.counter_name == counter), metric]\n",
    "        b = lb_detection_df.loc[(lb_detection_df.counter_name == counter), 'baseline']\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(y,x)\n",
    "        degree = 2\n",
    "        #coeffs = np.polyfit(x, y, degree)\n",
    "        # now, coeffs is an array which contains the polynomial coefficients\n",
    "        # in ascending order, i.e. x^0, x^1, x^2\n",
    "        #print('1:ndistance' + str(slope_df.loc[(lb_detection_df.counter_name == counter), 'ndistance']))\n",
    "        #intercept1, linear, quadratic = coeffs\n",
    "        slope_df.loc[(slope_df.counter_name == counter), 'slope'] = slope\n",
    "        #slope_df.loc[(slope_df.counter_name == counter), 'quad'] = quadratic\n",
    "        #print('2:ndistance' + str(slope_df.loc[(lb_detection_df.counter_name == counter), 'ndistance']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slope_df.loc[slope_df.load == 90,['counter_name','ndistance','slope', 'sbinary']].to_csv('eo.csv',index=False)\n",
    "slope_df.loc[slope_df.load == 90,['counter_name','ndistance','slope']].to_csv('eo2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meas_90_df = slope_df.loc[slope_df.load == 90,['ndistance','slope']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_meas_90_df = slope_df.loc[slope_df.load == 90,['ndistance','slope','counter_name','assessment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "ax.scatter(meas_90_df['ndistance'], meas_90_df['slope'], color='blue')\n",
    "ax.set_xlabel(' normalized distance > 1 failed performance requirement baseline, load = 90%')\n",
    "ax.set_ylabel('slope > 0.1 failed scalability trend')\n",
    "        #ax.set_title(\"{} vs {}\".format(x_col, y_col))\n",
    "majorLocator   = MultipleLocator(5)\n",
    "majorFormatter = FormatStrFormatter('%d')\n",
    "minorLocator   = MultipleLocator(1)\n",
    "    #plt.legend()\n",
    "#plt.show()\n",
    "plt.axhline(0.1,color='black')  #horizontal line\n",
    "plt.axvline(1,color='black')  #vertical line\n",
    "plt.grid()\n",
    "plt.savefig('partition_induced.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meas_90_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot with vertical lines per SPA\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.scatter(meas_90_df['ndistance'], (meas_90_df['slope']/meas_90_df['slope'].max()), color='blue')\n",
    "\n",
    "\n",
    "ax.set_xlabel(' normalized distance > 1 failed performance requirement baseline, load = 90%')\n",
    "ax.set_ylabel('nomalized slope')\n",
    "        #ax.set_title(\"{} vs {}\".format(x_col, y_col))\n",
    "\n",
    "# Prepare data for training\n",
    "spa_t_df = spa_df.copy()\n",
    "spa_t_df['index'] = pd.Series()\n",
    "max1=spa_t_df['slope'].max()\n",
    "plt.axvline(spa_t_df['ndistance'][0], color='magenta',label=spa_t_df['spa'][0]) \n",
    "plt.axvline(spa_t_df['ndistance'][1], color='purple',label=spa_t_df['spa'][1])\n",
    "plt.axvline(spa_t_df['ndistance'][2], color='green',label=spa_t_df['spa'][2])\n",
    "plt.axvline(spa_t_df['ndistance'][3], color='yellow',label=spa_t_df['spa'][3])\n",
    "plt.axvline(spa_t_df['ndistance'][4], color='blue',label=spa_t_df['spa'][4])\n",
    "plt.axvline(spa_t_df['ndistance'][5], color='orange',label=spa_t_df['spa'][5])\n",
    "plt.axvline(spa_t_df['ndistance'][6], color='red',label=spa_t_df['spa'][6])\n",
    "plt.axhline((spa_t_df['slope'][0]/spa_t_df['slope'].max()), color='magenta') \n",
    "plt.axhline((spa_t_df['slope'][1]/spa_t_df['slope'].max()), color='purple')\n",
    "plt.axhline((spa_t_df['slope'][2]/spa_t_df['slope'].max()), color='green')\n",
    "plt.axhline((spa_t_df['slope'][3]/spa_t_df['slope'].max()), color='yellow')\n",
    "plt.axhline((spa_t_df['slope'][4]/spa_t_df['slope'].max()), color='blue')\n",
    "plt.axhline((spa_t_df['slope'][5]/spa_t_df['slope'].max()), color='orange')\n",
    "plt.axhline((spa_t_df['slope'][6]/spa_t_df['slope'].max()), color='red')\n",
    "#plt.grid()\n",
    "ax.set_ylim(-0.2,1.1)\n",
    "ax.set_xlim(0,2)\n",
    "        #ax.set_title(\"{} vs {}\".format(x_col, y_col))\n",
    "majorLocator   = MultipleLocator(5)\n",
    "majorFormatter = FormatStrFormatter('%d')\n",
    "minorLocator   = MultipleLocator(1)\n",
    "plt.axhline(0.1,color='black')  #horizontal line\n",
    "plt.axvline(1,color='black')  #vertical line\n",
    "majorLocator   = MultipleLocator(5)\n",
    "majorFormatter = FormatStrFormatter('%d')\n",
    "minorLocator   = MultipleLocator(1)\n",
    "#plt.legend()\n",
    "#plt.show()\n",
    "plt.axhline(0.1,color='black')  #horizontal line\n",
    "plt.axvline(1,color='black')  #vertical line\n",
    "\n",
    "#ax.legend(fancybox=True, framealpha=1, shadow=True, borderpad=1, loc='upper left')\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.05),\n",
    "          ncol=2, fancybox=True, shadow=True)\n",
    "plt.legend()\n",
    "#plt.show()\n",
    "plt.savefig('partition.pdf')\n",
    "#plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot with vertical lines per SPA\n",
    "\n",
    "\n",
    "        #ax.set_title(\"{} vs {}\".format(x_col, y_col))\n",
    "\n",
    "# Prepare data for training\n",
    "spa_t_df = spa_df.copy()\n",
    "spa_t_df['index'] = pd.Series()\n",
    "max1=spa_t_df['slope'].max()\n",
    "for i in range(0,7):\n",
    "    ax = plt.gca()\n",
    "    ax.scatter(meas_90_df['ndistance'], (meas_90_df['slope']/meas_90_df['slope'].max()), color='blue')\n",
    "\n",
    "\n",
    "    ax.set_xlabel(' normalized distance > 1 failed performance requirement baseline, load = 90%')\n",
    "    ax.set_ylabel('nomalized slope')\n",
    "    plt.axvline(spa_t_df['ndistance'][i], color='orange',label=spa_t_df['spa'][i]) \n",
    "\n",
    "    plt.axhline((spa_t_df['slope'][i]/spa_t_df['slope'].max()), color='orange') \n",
    "\n",
    "    #plt.grid()\n",
    "    ax.set_ylim(-0.2,1.1)\n",
    "    ax.set_xlim(0,2)\n",
    "            #ax.set_title(\"{} vs {}\".format(x_col, y_col))\n",
    "    majorLocator   = MultipleLocator(5)\n",
    "    majorFormatter = FormatStrFormatter('%d')\n",
    "    minorLocator   = MultipleLocator(1)\n",
    "    plt.axhline(0.1,color='black')  #horizontal line\n",
    "    plt.axvline(1,color='black')  #vertical line\n",
    "    majorLocator   = MultipleLocator(5)\n",
    "    majorFormatter = FormatStrFormatter('%d')\n",
    "    minorLocator   = MultipleLocator(1)\n",
    "    #plt.legend()\n",
    "    #plt.show()\n",
    "    plt.axhline(0.1,color='black')  #horizontal line\n",
    "    plt.axvline(1,color='black')  #vertical line\n",
    "\n",
    "    #ax.legend(fancybox=True, framealpha=1, shadow=True, borderpad=1, loc='upper left')\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.05),\n",
    "              ncol=2, fancybox=True, shadow=True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only assessment = true counters are taken\n",
    "meas_90 = slope_df.loc[(slope_df.load == 90) &(slope_df.assessment== True), ['counter_name','ndistance','slope']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meas_90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spa_mean_std= pd.read_csv(\"spasmeanstd.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spa_mean_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_baseline(mean, std):\n",
    "    return (mean * 0.6 + std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spa_mean_std['baseline'] = spa_mean_std.apply(lambda x: calculate_baseline(x[\"mean\"], x[\"stddev\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spa_mean_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spa_mean_std=pd.concat([spa_mean_std, spa_t_df],axis=1)\n",
    "spa_mean_std=spa_mean_std.drop(['spa', 'index'], axis = 1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spa_mean_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meas_90=meas_90.reset_index()\n",
    "meas_90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm=meas_90.iloc[:,2:]\n",
    "ff=spa_mean_std.iloc[:,4:]\n",
    "mm.iloc[:,1]=mm.iloc[:,1]/mm.iloc[:,1].max()\n",
    "ff.iloc[:,1]=ff.iloc[:,1]/ff.iloc[:,1].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[1.00,0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_l=len(ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.loc[ff_l]=a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean distance\n",
    "ress=[]\n",
    "for i in range(0,7):\n",
    "    for j in range(0,8):\n",
    "        res=((((ff[\"ndistance\"][j] - mm[\"ndistance\"][i] )**2) + ((ff[\"slope\"][j]-mm[\"slope\"][i])**2) )**0.5)\n",
    "        ress.append(res)\n",
    "         \n",
    "ress=pd.Series(ress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Status_Updates= pd.concat([spa_mean_std['SPAs'],ress[0:8]],axis = 1)\n",
    "Status_Updates.rename(columns = {0:'euclidean_distance_Control'}, inplace = True) \n",
    "Status_Updates = Status_Updates.replace(np.nan, 'Base', regex=True)\n",
    "min_dist_indx=Status_Updates['euclidean_distance_Control'].idxmin\n",
    "min_dist_spa=Status_Updates['SPAs'][min_dist_indx]\n",
    "print('\\n',Status_Updates)\n",
    "print('The Status_Updates Service belongs to ',min_dist_spa)\n",
    "\n",
    "Control= pd.concat([spa_mean_std['SPAs'],ress[8:16]],axis = 1)\n",
    "Control.rename(columns = {0:'euclidean_distance_Control'}, inplace = True) \n",
    "Control = Control.replace(np.nan, 'Base', regex=True)\n",
    "#min_dist_indx=Control['euclidean_distance_Control'].idxmin\n",
    "#min_dist_spa=Control['SPAs'][min_dist_indx]\n",
    "#print('\\n',Control)\n",
    "#print('The Control Service belongs to ',min_dist_spa)\n",
    "\n",
    "DB_Data_Management= pd.concat([spa_mean_std['SPAs'],ress[16:24].reset_index(drop=True)],axis = 1,ignore_index=True)\n",
    "DB_Data_Management.rename(columns = {0:'SPAs',1:'euclidean_distance_Control'}, inplace = True) \n",
    "DB_Data_Management= DB_Data_Management.replace(np.nan, 'Base', regex=True)\n",
    "min_dist_indx=DB_Data_Management['euclidean_distance_Control'].idxmin\n",
    "min_dist_spa=DB_Data_Management['SPAs'][min_dist_indx]\n",
    "print('\\n',DB_Data_Management)\n",
    "print('The DB_Data_Management Service belongs to',min_dist_spa)\n",
    "\n",
    "Enquiry= pd.concat([spa_mean_std['SPAs'],ress[24:32].reset_index(drop=True)],axis = 1,ignore_index=True)\n",
    "Enquiry.rename(columns = {0:'SPAs',1:'euclidean_distance_Control'}, inplace = True) \n",
    "Enquiry= Enquiry.replace(np.nan, 'Base', regex=True)\n",
    "min_dist_indx=Enquiry['euclidean_distance_Control'].idxmin\n",
    "min_dist_spa=Enquiry['SPAs'][min_dist_indx]\n",
    "print('\\n',Enquiry)\n",
    "print('The Enquiry Service belongs to',min_dist_spa)\n",
    "\n",
    "Interrogation= pd.concat([spa_mean_std['SPAs'],ress[32:40].reset_index(drop=True)],axis = 1,ignore_index=True)\n",
    "Interrogation.rename(columns = {0:'SPAs',1:'euclidean_distance_Control'}, inplace = True) \n",
    "Interrogation= Interrogation.replace(np.nan, 'Base', regex=True)\n",
    "min_dist_indx=Interrogation['euclidean_distance_Control'].idxmin\n",
    "min_dist_spa=Interrogation['SPAs'][min_dist_indx]\n",
    "print('\\n',Interrogation)\n",
    "print('The Interrogation Service belongs to',min_dist_spa)\n",
    "\n",
    "\n",
    "Offline= pd.concat([spa_mean_std['SPAs'],ress[40:48].reset_index(drop=True)],axis = 1,ignore_index=True)\n",
    "Offline.rename(columns = {0:'SPAs',1:'euclidean_distance_Control'}, inplace = True) \n",
    "Offline= Offline.replace(np.nan, 'Base', regex=True)\n",
    "min_dist_indx=Offline['euclidean_distance_Control'].idxmin\n",
    "min_dist_spa=Offline['SPAs'][min_dist_indx]\n",
    "print('\\n',Offline)\n",
    "print('The Offline Service belongs to',min_dist_spa)\n",
    "\n",
    "Recompose= pd.concat([spa_mean_std['SPAs'],ress[48:56].reset_index(drop=True)],axis = 1,ignore_index=True)\n",
    "Recompose.rename(columns = {0:'SPAs',1:'euclidean_distance_Control'}, inplace = True) \n",
    "Recompose= Recompose.replace(np.nan, 'Base', regex=True)\n",
    "min_dist_indx=Recompose['euclidean_distance_Control'].idxmin\n",
    "min_dist_spa=Recompose['SPAs'][min_dist_indx]\n",
    "print('\\n',Recompose)\n",
    "print('The Recompose Service belongs to',min_dist_spa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=['Status_Updates',  'Control', 'DB Data Management',\n",
    "       'Enquiry', 'Interrogation', 'Recompose', 'Offline','Application hiccups', 'Continuous violated req.', 'Traffic jam',\n",
    "       'The stifle', 'Expensive DB call', 'Empty semi-trucks', 'The Blob','Base']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=pd.concat([mm,ff],axis=0)\n",
    "Y=Y.reset_index()\n",
    "Y['names']=names\n",
    "Y=Y.drop('index',axis=1)\n",
    "Y=Y[['names','ndistance','slope']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "x_test=Y.iloc[[0,1,2,3,4,5,6,7],1:].values\n",
    "xcv=Y.iloc[8:,1:].values\n",
    "\n",
    "idx_test = cdist(x_test,xcv,'euclidean','Smallest',1);\n",
    "idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "Z_names=Y.iloc[[0,1,2,3,4,5,6,7,12],0]\n",
    "Z=Y.iloc[[0,1,2,3,4,5,6,7,12],1:].values\n",
    "kmeans=KMeans(n_clusters=2,init='k-means++',random_state=0)\n",
    "y_kmeans=kmeans.fit_predict(Z)\n",
    "\n",
    "plt.scatter(Z[y_kmeans==0,0],Z[y_kmeans==0,1],s=100,c='red',label='Cluster1')\n",
    "plt.scatter(Z[y_kmeans==1,0],Z[y_kmeans==1,1],s=100,c='blue',label='Cluster2')\n",
    "print(Z_names)\n",
    "print(y_kmeans)\n",
    "\n",
    "\n",
    "plt.title('DBCALL normalized slope')\n",
    "plt.xlabel('nomalized distance')\n",
    "plt.ylabel('nomalized slope')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "Z_names=Y.iloc[[0,1,2,3,4,5,6,7,11],0]\n",
    "Z=Y.iloc[[0,1,2,3,4,5,6,7,11],1:].values\n",
    "\n",
    "kmeans=KMeans(n_clusters=2,init='k-means++',random_state=0)\n",
    "y_kmeans=kmeans.fit_predict(Z)\n",
    "\n",
    "#Visualize the clusters\n",
    "\n",
    "plt.scatter(Z[y_kmeans==0,0],Z[y_kmeans==0,1],s=100,c='red',label='Cluster1')\n",
    "plt.scatter(Z[y_kmeans==1,0],Z[y_kmeans==1,1],s=100,c='blue',label='Cluster2')\n",
    "\n",
    "print(Z_names)\n",
    "print(y_kmeans)\n",
    "plt.title('Stiffle normalized slope')\n",
    "plt.xlabel('nomalized distance')\n",
    "plt.ylabel('nomalized slope')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPA values convert to arr and given as centroid\n",
    "xcv=Y.iloc[7:,1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Z=Y.iloc[:,1:].values\n",
    "kmeans=KMeans(n_clusters=8,init=xcv,random_state=0)\n",
    "y_kmeans=kmeans.fit_predict(Z)\n",
    "x_x=kmeans.cluster_centers_\n",
    "\n",
    "plt.scatter(Z[y_kmeans==0,0],Z[y_kmeans==0,1],s=100,c='red',label=spa_t_df['spa'][0])\n",
    "plt.scatter(Z[y_kmeans==1,0],Z[y_kmeans==1,1],s=100,c='blue',label=spa_t_df['spa'][1])\n",
    "plt.scatter(Z[y_kmeans==2,0],Z[y_kmeans==2,1],s=100,c='orange',label=spa_t_df['spa'][2])\n",
    "plt.scatter(Z[y_kmeans==3,0],Z[y_kmeans==3,1],s=100,c='brown',label=spa_t_df['spa'][3])\n",
    "plt.scatter(Z[y_kmeans==4,0],Z[y_kmeans==4,1],s=100,c='pink',label=spa_t_df['spa'][4])\n",
    "plt.scatter(Z[y_kmeans==5,0],Z[y_kmeans==5,1],s=100,c='purple',label=spa_t_df['spa'][5])\n",
    "plt.scatter(Z[y_kmeans==6,0],Z[y_kmeans==6,1],s=100,c='brown',label=spa_t_df['spa'][6])\n",
    "#plt.scatter(Z[y_kmeans==7,0],Z[y_kmeans==7,1],s=100,c='c',label='Base')\n",
    "plt.scatter(xcv[0:7,0],xcv[0:7,1], marker = \"x\", s = 50, color = 'black')\n",
    "\n",
    "plt.scatter(Y['ndistance'][0],Y['slope'][0],s=20,marker=\".\",c='gray',label='Status_Updates')\n",
    "plt.scatter(Y['ndistance'][1],Y['slope'][1],s=20,marker=\".\",c='brown',label='Control')\n",
    "plt.scatter(Y['ndistance'][2],Y['slope'][2],s=20,marker=\".\",c='cyan',label='DB Data Management')\n",
    "plt.scatter(Y['ndistance'][3],Y['slope'][3],s=20,marker=\".\",c='red',label='Enquiry')\n",
    "plt.scatter(Y['ndistance'][4],Y['slope'][4],s=20,marker=\".\",c='yellow',label='Interrogation')\n",
    "plt.scatter(Y['ndistance'][5],Y['slope'][5],s=20,marker=\".\",c='green',label='Offline')\n",
    "plt.scatter(Y['ndistance'][6],Y['slope'][6],s=20,marker=\".\",c='black',label='Recompose')\n",
    "\n",
    "#plt.title('SPAs and Services')\n",
    "plt.xlabel('nomalized distance')\n",
    "plt.ylabel('nomalized slope')\n",
    "\n",
    "# Put a legend below current axis\n",
    "#ax.legend(loc='upper center', \n",
    "     #     , shadow=True, ncol=5)\n",
    "\n",
    "#plt.legend(loc='lower center', ncol=1,fancybox=True,bbox_to_anchor=(0.3, 0.3))\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "#plt.rcParams[\"figure.figsize\"] = (6,6)\n",
    "plt.savefig('kmeans_legend.pdf',dpi=300)\n",
    "plt.savefig('kmeans_legend.jpeg',dpi=300)\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_centroids=dict()\n",
    "clusters_radii= dict()\n",
    "\n",
    "'''looping over clusters and calculate Euclidian distance of \n",
    "each point within that cluster from its centroid and \n",
    "pick the maximum which is the radius of that cluster'''\n",
    "\n",
    "for cluster in range(0,8):\n",
    "\n",
    "    clusters_centroids[cluster]=list(zip(kmeans.cluster_centers_[:, 0],kmeans.cluster_centers_[:,1]))[cluster]\n",
    "    clusters_radii[cluster] = max([np.linalg.norm(np.subtract(i,clusters_centroids[cluster])) for i in zip(Z[y_kmeans == cluster, 0],Z[y_kmeans == cluster, 1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "fig, ax = plt.subplots(1,figsize=(7,5))\n",
    "\n",
    "plt.scatter(Z[y_kmeans==0,0],Z[y_kmeans==0,1],s=100,c='red',label=spa_t_df['spa'][0])\n",
    "art = mpatches.Circle(clusters_centroids[0],clusters_radii[0], edgecolor='r',fill=False)\n",
    "ax.add_patch(art)\n",
    "\n",
    "plt.scatter(Z[y_kmeans==1,0],Z[y_kmeans==1,1],s=100,c='blue',label=spa_t_df['spa'][1])\n",
    "art = mpatches.Circle(clusters_centroids[1],clusters_radii[1], edgecolor='b',fill=False)\n",
    "ax.add_patch(art)\n",
    "\n",
    "plt.scatter(Z[y_kmeans==2,0],Z[y_kmeans==2,1],s=100,c='orange',label=spa_t_df['spa'][2])\n",
    "art = mpatches.Circle(clusters_centroids[2],clusters_radii[2], edgecolor='orange',fill=False)\n",
    "ax.add_patch(art)\n",
    "\n",
    "\n",
    "plt.scatter(Z[y_kmeans==3,0],Z[y_kmeans==3,1],s=100,c='green',label=\"Outlier\")\n",
    "art = mpatches.Circle(clusters_centroids[3],clusters_radii[3], edgecolor='g',fill=False)\n",
    "ax.add_patch(art)\n",
    "\n",
    "\n",
    "plt.scatter(Z[y_kmeans==4,0],Z[y_kmeans==4,1],s=100,c='pink',label=spa_t_df['spa'][4])\n",
    "art = mpatches.Circle(clusters_centroids[4],clusters_radii[4], edgecolor='g',fill=False)\n",
    "ax.add_patch(art)\n",
    "\n",
    "\n",
    "plt.scatter(Z[y_kmeans==5,0],Z[y_kmeans==5,1],s=100,c='purple',label=spa_t_df['spa'][5])\n",
    "art = mpatches.Circle(clusters_centroids[5],clusters_radii[5], edgecolor='purple',fill=False)\n",
    "ax.add_patch(art)\n",
    "\n",
    "plt.scatter(Z[y_kmeans==6,0],Z[y_kmeans==6,1],s=100,c='brown',label=spa_t_df['spa'][6])\n",
    "art = mpatches.Circle(clusters_centroids[6],clusters_radii[6], edgecolor='brown',fill=False)\n",
    "ax.add_patch(art)\n",
    "\n",
    "#plt.scatter(Z[y_kmeans==7,0],Z[y_kmeans==7,1],s=100,c='c',label='Base')\n",
    "#art = mpatches.Circle(clusters_centroids[7],clusters_radii[7], edgecolor='g',fill=False)\n",
    "#ax.add_patch(art)\n",
    "\n",
    "#plt.scatter(xcv[:,0],xcv[:,1], marker = \"x\", s = 50, color = 'w', label = 'SPA Coordinates')\n",
    "\n",
    "plt.scatter(Y['ndistance'][0],Y['slope'][0],s=20,marker=\".\",c='gray',label='Status_Updates')\n",
    "plt.scatter(Y['ndistance'][1],Y['slope'][1],s=20,marker=\".\",c='brown',label='Control')\n",
    "plt.scatter(Y['ndistance'][2],Y['slope'][2],s=20,marker=\".\",c='cyan',label='DB Data Management')\n",
    "plt.scatter(Y['ndistance'][3],Y['slope'][3],s=20,marker=\".\",c='red',label='Enquiry')\n",
    "plt.scatter(Y['ndistance'][4],Y['slope'][4],s=20,marker=\".\",c='yellow',label='Interrogation')\n",
    "plt.scatter(Y['ndistance'][5],Y['slope'][5],s=20,marker=\".\",c='green',label='Offline')\n",
    "plt.scatter(Y['ndistance'][6],Y['slope'][6],s=20,marker=\".\",c='black',label='Recompose')\n",
    "#plt.scatter(Z[y_kmeans==7,0],Z[y_kmeans==7,1],s=100,c='c',label='Base')\n",
    "plt.scatter(xcv[0:7,0],xcv[0:7,1], marker = \"x\", s = 50, color = 'black')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('kmeans_centroid.pdf',dpi=300)\n",
    "plt.savefig('kmeans_centroid.jpeg',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans=KMeans(n_clusters=8,init='k-means++', n_init=10,random_state=0)\n",
    "y_kmeans=kmeans.fit_predict(Z)\n",
    "\n",
    "plt.scatter(Z[y_kmeans==0,0],Z[y_kmeans==0,1],s=100,c='red')\n",
    "plt.scatter(Z[y_kmeans==1,0],Z[y_kmeans==1,1],s=100,c='blue')\n",
    "plt.scatter(Z[y_kmeans==2,0],Z[y_kmeans==2,1],s=100,c='orange')\n",
    "plt.scatter(Z[y_kmeans==3,0],Z[y_kmeans==3,1],s=100,c='green')\n",
    "plt.scatter(Z[y_kmeans==4,0],Z[y_kmeans==4,1],s=100,c='pink')\n",
    "plt.scatter(Z[y_kmeans==5,0],Z[y_kmeans==5,1],s=100,c='purple')\n",
    "plt.scatter(Z[y_kmeans==6,0],Z[y_kmeans==6,1],s=100,c='brown')\n",
    "plt.scatter(Z[y_kmeans==7,0],Z[y_kmeans==7,1],s=100,c='yellow')\n",
    "\n",
    "\n",
    "plt.title('SPAs and Services')\n",
    "plt.xlabel('nomalized distance')\n",
    "plt.ylabel('nomalized slope')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot with vertical lines per SPA\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.scatter(meas_90_df['ndistance'], meas_90_df['slope'], color='blue')\n",
    "\n",
    "\n",
    "ax.set_xlabel(' normalized distance > 1 failed performance requirement baseline, load = 90%')\n",
    "ax.set_ylabel('slope > 0.1 failed scalability trend')\n",
    "        #ax.set_title(\"{} vs {}\".format(x_col, y_col))\n",
    "\n",
    "# Prepare data for training\n",
    "spa_t_df = spa_df.copy()\n",
    "spa_t_df['index'] = pd.Series()\n",
    "max1=spa_t_df['slope'].max()\n",
    "plt.axvline(spa_t_df['ndistance'][0], color='magenta',label=spa_t_df['spa'][0]) \n",
    "plt.axvline(spa_t_df['ndistance'][1], color='purple',label=spa_t_df['spa'][1])\n",
    "plt.axvline(spa_t_df['ndistance'][2], color='green',label=spa_t_df['spa'][2])\n",
    "plt.axvline(spa_t_df['ndistance'][3], color='yellow',label=spa_t_df['spa'][3])\n",
    "plt.axvline(spa_t_df['ndistance'][4], color='blue',label=spa_t_df['spa'][4])\n",
    "plt.axvline(spa_t_df['ndistance'][5], color='orange',label=spa_t_df['spa'][5])\n",
    "plt.axvline(spa_t_df['ndistance'][6], color='red',label=spa_t_df['spa'][6])\n",
    "#plt.grid()\n",
    "ax.set_ylim(-30,350)\n",
    "ax.set_xlim(0,2)\n",
    "        #ax.set_title(\"{} vs {}\".format(x_col, y_col))\n",
    "majorLocator   = MultipleLocator(5)\n",
    "majorFormatter = FormatStrFormatter('%d')\n",
    "minorLocator   = MultipleLocator(1)\n",
    "plt.axhline(0.1,color='black')  #horizontal line\n",
    "plt.axvline(1,color='black')  #vertical line\n",
    "majorLocator   = MultipleLocator(5)\n",
    "majorFormatter = FormatStrFormatter('%d')\n",
    "minorLocator   = MultipleLocator(1)\n",
    "#plt.legend()\n",
    "#plt.show()\n",
    "plt.axhline(0.1,color='black')  #horizontal line\n",
    "plt.axvline(1,color='black')  #vertical line\n",
    "\n",
    "#ax.legend(fancybox=True, framealpha=1, shadow=True, borderpad=1, loc='upper left')\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.05),\n",
    "          ncol=2, fancybox=True, shadow=True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for load in lb_detection_df.load.unique():\n",
    " #   if (load == 90):\n",
    "       # for counter in lb_detection_df.loc[lb_detection_df.counter_name == service, 'counter_name'].unique():\n",
    "figure = plt.figure\n",
    "ax = plt.gca()\n",
    "ax.scatter(slope_df['ndistance'], slope_df['slope'], color='red')\n",
    "ax.set_xlabel(' normalized distance > 1 failed performance requirement baseline')\n",
    "ax.set_ylabel('slope > 0.1 failed scalability trend')\n",
    "        #ax.set_title(\"{} vs {}\".format(x_col, y_col))\n",
    "majorLocator   = MultipleLocator(5)\n",
    "majorFormatter = FormatStrFormatter('%d')\n",
    "minorLocator   = MultipleLocator(1)\n",
    "    #plt.legend()\n",
    "#plt.show()\n",
    "plt.axhline(0.1,color='black')  #horizontal line\n",
    "plt.axvline(1,color='black')  #vertical line\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.set(style=\"whitegrid\")\n",
    "sns.set()\n",
    "sns.relplot(x='ndistance',y='slope',\n",
    "              hue=\"counter_name\",size=\"load\",\n",
    "              data=slope_df, style=\"assessment\",legend=\"full\" )\n",
    "              #data=full_meas_90_df, style=\"assessment\",legend=\"full\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#full_meas_90_df\n",
    "sns.set()\n",
    "sns.relplot(x='ndistance',y='slope',\n",
    "          hue=\"counter_name\", style=\"assessment\",\n",
    "            data=full_meas_90_df,legend=\"full\")\n",
    "plt.savefig('full_meas_90.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distances and plot slope vs. distance\n",
    "for service in lb_detection_df.counter_name.unique():\n",
    "    for counter in lb_detection_df.loc[lb_detection_df.counter_name == service, 'counter_name'].unique():\n",
    "        figure = plt.figure\n",
    "        ax = plt.gca()\n",
    "        ax.scatter(slope_df['distance'], slope_df['slope'])\n",
    "        ax.set_xlabel('distance')\n",
    "        ax.set_ylabel('slope')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_distance_df = pd.DataFrame(df.groupby(['counter_name']).assessment.any())\n",
    "failed_distance_df.columns.name=None\n",
    "failed_distance_df=failed_distance_df.reset_index()\n",
    "failed_distance_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for counter in failed_distance_df.loc[failed_distance_df.assessment == True,'counter_name'].values:\n",
    "    plt.figure()\n",
    "    x = df.loc[df.counter_name == counter, 'load']\n",
    "    y = df.loc[df.counter_name == counter, metric]\n",
    "    b = df.loc[df.counter_name == counter, 'baseline']\n",
    "    plt.xlabel(str(counter) + ' MAX FAILED ')\n",
    "    plt.scatter(x, y, s=10, color='green')\n",
    "    plt.plot(x,b,color='red',label='baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models (one per counter, load as the second dimension of the anomaly detection process)\n",
    "for service in lb_detection_df.counter_name.unique():\n",
    "    for counter in lb_detection_df.loc[lb_detection_df.counter_name == service, 'counter_name'].unique():\n",
    "        x = lb_detection_df.loc[(lb_detection_df.counter_name == counter), 'load']\n",
    "        y = lb_detection_df.loc[(lb_detection_df.counter_name == counter), metric]\n",
    "        b = lb_detection_df.loc[(lb_detection_df.counter_name == counter), 'baseline']\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(y,x)\n",
    "        print('counter = ' + str(counter) + ' slope = ' + str(slope))\n",
    "        if (slope < 0.01):\n",
    "            plt.figure()\n",
    "            print('p_value = '+ str(p_value))\n",
    "            print('slope = ' + str(slope))\n",
    "            print('intercept = ' +str(intercept))\n",
    "            print('r_value = ' +str(r_value))\n",
    "            print('std_err = ' +str(std_err))\n",
    "            x = lb_detection_df.loc[(lb_detection_df.counter_name == counter), 'load']\n",
    "            y = lb_detection_df.loc[(lb_detection_df.counter_name == counter), metric]\n",
    "            plt.xlabel(str(counter) + '\\n SLOPE PASS -- p_value = ' + str(p_value) + '   slope = '+ str(slope) + ' r_value = ' + str(r_value))\n",
    "            plt.scatter(x, y, s=10, color='green')\n",
    "            plt.plot(x,b,color='red',label='baseline')\n",
    "\n",
    "        \n",
    "        if (slope >= 0.01) and (p_value < 0.05):\n",
    "            plt.figure()\n",
    "            x = lb_detection_df.loc[(lb_detection_df.counter_name == counter), 'load']\n",
    "            y = lb_detection_df.loc[(lb_detection_df.counter_name == counter), metric]\n",
    "            plt.xlabel(str(counter) + '\\n SLOPE FAIL p_value = ' + str(p_value) + '   slope = '+ str(slope) + ' r_value = ' + str(r_value))\n",
    "            plt.scatter(x, y, s=10, color='green')\n",
    "            plt.plot(x,b,color='red',label='baseline')\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=spa_t_df['ndistance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics \n",
    "\n",
    "data=spa_t_df['slope']\n",
    "# Function to Detection Outlier on one-dimentional datasets.\n",
    "def find_anomalies(data):\n",
    "    #define a list to accumlate anomalies\n",
    "    anomalies = []\n",
    "    \n",
    "    # Set upper and lower limit to 3 standard deviation\n",
    "    data_std = statistics.stdev(data)\n",
    "    data_mean = statistics.mean(data)\n",
    "    anomaly_cut_off = data_std * 3\n",
    "    print(anomaly_cut_off)\n",
    "    lower_limit  = data_mean - anomaly_cut_off \n",
    "    upper_limit = data_mean + anomaly_cut_off\n",
    "    print(lower_limit)\n",
    "    print(upper_limit)\n",
    "    # Generate outliers\n",
    "    for outlier in data:\n",
    "        if outlier > upper_limit or outlier < lower_limit:\n",
    "            print('hi')\n",
    "            anomalies.append(outlier)\n",
    "    return anomalies\n",
    "\n",
    "find_anomalies(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "data=spa_t_df['ndistance']\n",
    "sns.boxplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=spa_t_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataa= data.drop(['spa', 'index'], axis = 1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.boxplot(data=spa_t_df['slope'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(spa_t_df['ndistance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats.shapiro((spa_t_df['slope'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import pylab\n",
    "sm.qqplot(spa_t_df['slope'], loc = 4, scale = 3, line='s')\n",
    "pylab.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
